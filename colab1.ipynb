{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yQDqj3jXkXk"
      },
      "source": [
        "**Predicting compressive strength of concrete mixture using neural networks**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8wJuUBIX2Ld"
      },
      "source": [
        "This cell is used to get the code from google drive and move to project directory (required)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YF4FebkqJP4Z",
        "outputId": "2da63858-ab24-4f25-d8cb-bd838a6dbbb4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/concrete_ai')\n",
        "cwd = os.getcwd()\n",
        "print('current directory : ', cwd)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "current directory :  /content/drive/MyDrive/concrete_ai\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SjWROrMbCCJ"
      },
      "source": [
        "Importing the python packages (required)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIm3ayl8IKkf"
      },
      "source": [
        "# importing neccessary packages\n",
        "import os\n",
        "import data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from utils import utils\n",
        "import tensorflow as tf\n",
        "import printing_callback as callback"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuyHDN7MhH7g"
      },
      "source": [
        "**Setting up the network structure**\n",
        "\n",
        "this cell sets up the neural network structure global variables\n",
        "the variable are:\n",
        "\n",
        "\n",
        "1.   NUM_EPOCHS : the number of trainning epochs\n",
        "2.   SAVE_INPUT : saves the prepared input\n",
        "3.   SAVE_OUTPUT : saves the input + the predicions\n",
        "4.   SAVE_WEIGHTS : saves the weights of the model after the trainning finishes\n",
        "5.   LOAD_WEIGHTS : loads the weights of the previously saved models\n",
        "6.   INPUT_TITLE : the file that contains the input data as excel file\n",
        "7.   OUTPUT_TITLE : the file that the output will be saved to\n",
        "8.  PRINT_PRDECTIONS : decides if should print random 20 predictions against their real data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecgMLiLBoRi-"
      },
      "source": [
        "# setting up the network structure\n",
        "NUM_EPOCHS = 30000\n",
        "SAVE_INPUT = False\n",
        "SAVE_OUTPUT = True\n",
        "SAVE_WEIGHTS = False\n",
        "LOAD_WEIGHTS = False\n",
        "INPUT_TITLE = 'amj_data3'\n",
        "OUTPUT_TITLE = 'amj_data'\n",
        "PRINT_PRDECTIONS = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C87-dyPIdjUm"
      },
      "source": [
        "**Model structure**\n",
        "\n",
        "The models is divided into three models :\n",
        "\n",
        "1.   Model1 : contains the sump and hardened density of concrete\n",
        "2.   Model2 : contains the concrete compressive strength after 7 days\n",
        "3.   Model3 : contains the concrete compressive strength after 28 days\n",
        "\n",
        "every one of these models are independednt from the other\n",
        "\n",
        "each one will have a sepereate cell for setting the data and trainning\n",
        "\n",
        "you will have to run the setting the data for the model you want to train first"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9w7fpXpe85G"
      },
      "source": [
        "**Model 1 (slump and hardened density)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKDxfkNcoby-",
        "outputId": "accf7db5-4611-4dae-eb74-a1aad506a248"
      },
      "source": [
        "train1, test1, validation1 = data.getFinalData('data_files/' + INPUT_TITLE + '.xlsx')\n",
        "train1, test1, validation1 = data.prepareMultipleData(train1, test1, validation1, [16,17])\n",
        "print('\\n train1:', train1.shape)\n",
        "xr1 = train1[:,0:14]\n",
        "yr1 = train1[:,14:16]\n",
        "xt1 = test1[:,0:14]\n",
        "yt1 = test1[:,14:16]\n",
        "xv1 = validation1[:,0:14]\n",
        "yv1 = validation1[:,14:16]\n",
        "utils.exceptionIfNan(train1)\n",
        "utils.exceptionIfNan(test1)\n",
        "utils.exceptionIfNan(validation1)\n",
        "print('data 1 ready with train:', train1.shape, 'and test:', test1.shape, 'and validation:', \n",
        "validation1.shape)\n",
        "if(SAVE_INPUT):\n",
        "    data.saveData(train1, 'train1.xlsx')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading the data ...\n",
            "data loaded with size:  (1063, 18) \n",
            "\n",
            "\n",
            "there is Nan in data\n",
            "\n",
            "\n",
            "shuffling the data ...\n",
            "data shuffled\n",
            "\n",
            "\n",
            "dividing the data ...\n",
            "training data ready with size: (745, 18)\n",
            "test data ready with size: (160, 18)\n",
            "validation data ready with size: (158, 18)\n",
            "\n",
            " train1: (742, 16)\n",
            "data 1 ready with train: (742, 16) and test: (160, 16) and validation: (158, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8ucMwcafKMV"
      },
      "source": [
        "**Model 2 (compressive strength after 7 days)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8g2DZhRogCH",
        "outputId": "8efb0923-d299-45c3-876b-fbc567712444"
      },
      "source": [
        "train2, test2, validation2 = data.getFinalData('data_files/' + INPUT_TITLE + '.xlsx')\n",
        "train2, test2, validation2 = data.prepareMultipleData(train2, test2, validation2, [14, 15, 17])\n",
        "xr2 = train2[:,0:9]\n",
        "yr2 = train2[:,14:15]\n",
        "xt2 = test2[:,0:9]\n",
        "yt2 = test2[:,14:15]\n",
        "xv2 = validation2[:,0:9]\n",
        "yv2 = validation2[:,14:15]\n",
        "utils.exceptionIfNan(train2)\n",
        "utils.exceptionIfNan(test2)\n",
        "utils.exceptionIfNan(validation2)\n",
        "print('data 2 ready with train:', train2.shape, 'and test:', test2.shape, 'and validation:', \n",
        "validation2.shape)\n",
        "if(SAVE_INPUT):\n",
        "    data.saveData(train2, 'train2.xlsx')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading the data ...\n",
            "data loaded with size:  (1063, 18) \n",
            "\n",
            "\n",
            "there is Nan in data\n",
            "\n",
            "\n",
            "shuffling the data ...\n",
            "data shuffled\n",
            "\n",
            "\n",
            "dividing the data ...\n",
            "training data ready with size: (745, 18)\n",
            "test data ready with size: (160, 18)\n",
            "validation data ready with size: (158, 18)\n",
            "data 2 ready with train: (742, 15) and test: (159, 15) and validation: (154, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhgVUKpEfPt2"
      },
      "source": [
        "**Model 3 (compressive strength after 28 days)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZV6JK6fol0j",
        "outputId": "d206ebfa-7aa8-4091-fb8e-013d926448a4"
      },
      "source": [
        "train3, test3, validation3 = data.getFinalData('data_files/' + INPUT_TITLE + '.xlsx')\n",
        "train3, test3, validation3 = data.prepareMultipleData(train3, test3, validation3, [14, 15, 16])\n",
        "xr3 = train3[:,0:14]\n",
        "yr3 = train3[:,14:15]\n",
        "xt3 = test3[:,0:14]\n",
        "yt3 = test3[:,14:15]\n",
        "xv3 = validation3[:,0:14]\n",
        "yv3 = validation3[:,14:15]\n",
        "utils.exceptionIfNan(train3)\n",
        "utils.exceptionIfNan(test3)\n",
        "utils.exceptionIfNan(validation3)\n",
        "print('data 3 ready with train:', train3.shape, 'and test:', test3.shape, 'and validation:', \n",
        "validation3.shape)\n",
        "if(SAVE_INPUT):\n",
        "    data.saveData(train3, 'train3.xlsx')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading the data ...\n",
            "data loaded with size:  (1063, 18) \n",
            "\n",
            "\n",
            "there is Nan in data\n",
            "\n",
            "\n",
            "shuffling the data ...\n",
            "data shuffled\n",
            "\n",
            "\n",
            "dividing the data ...\n",
            "training data ready with size: (745, 18)\n",
            "test data ready with size: (160, 18)\n",
            "validation data ready with size: (158, 18)\n",
            "data 3 ready with train: (361, 15) and test: (77, 15) and validation: (73, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgTB-7YffrI"
      },
      "source": [
        "**Trainning and testing the models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgefE72cfjEJ"
      },
      "source": [
        "**Model 1 (slump and hardened density)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zznNUShUopYC"
      },
      "source": [
        "model1 = utils.newSeqentialModel(14, 2)\n",
        " \n",
        "if(LOAD_WEIGHTS):\n",
        "    print('loading model weights ...')\n",
        "    output_dir = os.path.join(os.getcwd(), \"saved_wights\")\n",
        "    try:\n",
        "      model1.load_weights(filepath=os.path.join(output_dir, OUTPUT_TITLE + \"1.h5\"))\n",
        "      print('model weights loaded')\n",
        "    except OSError:\n",
        "      print('no previous weights found')\n",
        "    except ValueError:\n",
        "      print('previous weights are different from current') \n",
        " \n",
        "print('\\n\\nTrainning model 1')\n",
        "model1.compile(loss='mean_squared_error', optimizer='adam',\n",
        "metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n",
        "model1.fit(xr1, yr1, epochs=NUM_EPOCHS, batch_size=32, validation_data=(xv1, yv1),\n",
        "verbose=0, callbacks=[callback.LossAndErrorPrintingCallback()])\n",
        " \n",
        "_, accuracy_test_1 = model1.evaluate(xt1, yt1)\n",
        "print('\\n\\nmodel 1 trained')\n",
        "print('\\nAccuracy on test data: %.2f' % (accuracy_test_1))\n",
        " \n",
        "test_predictions = np.around(model1.predict(xt1), 1)\n",
        " \n",
        "if (PRINT_PRDECTIONS):\n",
        "    for i in range(20):\n",
        "        print(' test: predicted:', test_predictions[i], 'real data:', yt1[i])\n",
        " \n",
        "if (SAVE_OUTPUT):\n",
        "    result = np.concatenate([xt1, yt1, test_predictions], axis=1)\n",
        "    data.saveData(result, 'results/' + OUTPUT_TITLE + '1.xlsx')\n",
        " \n",
        "if (SAVE_WEIGHTS):\n",
        "    print('\\nsaving model weights ...')\n",
        "    output_dir = os.path.join(os.getcwd(), \"saved_wights\")\n",
        "    model1.save_weights(filepath=os.path.join(output_dir, OUTPUT_TITLE + \"1.h5\"))\n",
        "    print('model weights saved')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70iDUMl2SVz0"
      },
      "source": [
        "last output :\n",
        "\n",
        "trainning: mse: 1322.47 %: 12.97\n",
        "\n",
        "validation: mse : 1383.56 %: 12.97\n",
        "\n",
        "test data: %: 12.97\n",
        "\n",
        "converged: false\n",
        "\n",
        "data type: amj_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egE-BFDxfrQZ"
      },
      "source": [
        "**Model 2 (compressive strength after 7 days)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pdz96-A-oy-t"
      },
      "source": [
        "model2 = utils.newSeqentialModel(9, 1)\n",
        " \n",
        "if(LOAD_WEIGHTS):\n",
        "    print('loading model weights ...')\n",
        "    output_dir = os.path.join(os.getcwd(), \"saved_wights\")\n",
        "    try:\n",
        "      model2.load_weights(filepath=os.path.join(output_dir, OUTPUT_TITLE + \"2.h5\"))\n",
        "      print('model weights loaded')\n",
        "    except OSError:\n",
        "      print('no previous weights found')\n",
        "    except ValueError:\n",
        "      print('previous weights are different from current') \n",
        " \n",
        "print('\\n\\nTrainning model 2')\n",
        "model2.compile(loss='mean_squared_error', optimizer='adam',\n",
        "metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n",
        "model2.fit(xr2, yr2, epochs=NUM_EPOCHS, batch_size=32, validation_data=(xv2, yv2), \n",
        "verbose=0, callbacks=[callback.LossAndErrorPrintingCallback()])\n",
        " \n",
        "_, accuracy_test_2 = model2.evaluate(xt2, yt2)\n",
        "print('\\n\\nmodel 2 trained')\n",
        "print('\\nAccuracy on test data: %.2f' % (accuracy_test_2))\n",
        " \n",
        "test_predictions = np.around(model2.predict(xt2), 1)\n",
        " \n",
        "if (PRINT_PRDECTIONS):\n",
        "    for i in range(20):\n",
        "        print(' test: predicted:', test_predictions[i], 'real data:', yt2[i])\n",
        " \n",
        "if (SAVE_OUTPUT):\n",
        "    result = np.concatenate([xt2, yt2, test_predictions], axis=1)\n",
        "    data.saveData(result, 'results/' + OUTPUT_TITLE + '2.xlsx')\n",
        " \n",
        "if (SAVE_WEIGHTS):\n",
        "    print('\\nsaving model weights ...')\n",
        "    output_dir = os.path.join(os.getcwd(), \"saved_wights\")\n",
        "    model2.save_weights(filepath=os.path.join(output_dir, OUTPUT_TITLE + \"2.h5\"))\n",
        "    print('model weights saved')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WGKzoURQTrL"
      },
      "source": [
        "last output :\n",
        "\n",
        "trainning: mse: 29.52 %: 17.00\n",
        "\n",
        "validation: mse : 25.40 %: 17.00\n",
        "\n",
        "test data: %: 17.00\n",
        "\n",
        "converged: false\n",
        "\n",
        "data type: amj_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqHVmF4YfwDH"
      },
      "source": [
        "Model 3 (compressive strength after 28 days)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArfN2aRFo2IV"
      },
      "source": [
        "model3 = utils.newSeqentialModel(14, 1)\n",
        "\n",
        "if(LOAD_WEIGHTS):\n",
        "\tprint('loading model weights ...')\n",
        "\toutput_dir = os.path.join(os.getcwd(), \"saved_wights\")\n",
        "  try:\n",
        "    model3.load_weights(filepath=os.path.join(output_dir, OUTPUT_TITLE  +\"3.h5\"))\n",
        "    print('model weights loaded')\n",
        "  except OSError:\n",
        "    print('no previous weights found')\n",
        "  except ValueError:\n",
        "    print('previous weights are different from current') \n",
        "\n",
        "print('\\n\\nTrainning model 3')\n",
        "model3.compile(loss='mean_squared_error', optimizer='adam', \n",
        "metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n",
        "model3.fit(xr3, yr3, epochs=NUM_EPOCHS, batch_size=64, validation_data=(xv3, yv3), \n",
        "verbose=0, callbacks=[callback.LossAndErrorPrintingCallback()])\n",
        "\n",
        "_, accuracy_test_3 = model3.evaluate(xt3, yt3)\n",
        "print('\\n\\nmodel 3 trained')\n",
        "print('\\nAccuracy on test data: %.2f' % (accuracy_test_3))\n",
        "\n",
        "test_predictions = np.around(model3.predict(xt3), 1)\n",
        "\n",
        "if (PRINT_PRDECTIONS):\n",
        "\tfor i in range(20):\n",
        "\t\tprint(' test: predicted:', test_predictions[i], 'real data:', yt3[i])\n",
        "\n",
        "if (SAVE_OUTPUT):\n",
        "\tresult = np.concatenate([xt3, yt3, test_predictions], axis=1)\n",
        "\tdata.saveData(result, 'results/' + OUTPUT_TITLE + '3.xlsx')\n",
        "\n",
        "if (SAVE_WEIGHTS):\n",
        "\tprint('\\nsaving model weights ...')\n",
        "\toutput_dir = os.path.join(os.getcwd(), \"saved_wights\")\n",
        "\tmodel3.save_weights(filepath=os.path.join(output_dir, OUTPUT_TITLE + \"3.h5\"))\n",
        "\tprint('model weights saved')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBHQQSusP1kj"
      },
      "source": [
        "last output : \n",
        "\n",
        "trainning: mse: 17.32      %: 8.05\n",
        "\n",
        "validation: mse : 56.69     %: 15.29\n",
        "\n",
        "test data: %: 17.19\n",
        "\n",
        "converged: true"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCRu4x4tOgo6"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression,  Lasso, Ridge\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score \n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW5SJJB_fvA-"
      },
      "source": [
        "def mean_absolute_percentage_error(y_true, y_pred,\n",
        "                                   sample_weight=None,\n",
        "                                   multioutput='uniform_average'):\n",
        "    epsilon = np.finfo(np.float64).eps\n",
        "    mape = np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), epsilon)\n",
        "    output_errors = np.average(mape,\n",
        "                               weights=sample_weight, axis=0)\n",
        "    if isinstance(multioutput, str):\n",
        "        if multioutput == 'raw_values':\n",
        "            return output_errors\n",
        "        elif multioutput == 'uniform_average':\n",
        "            multioutput = None\n",
        "    return np.average(output_errors, weights=multioutput)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDHoSXRuOiVw"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore',category = DeprecationWarning)\n",
        "warnings.filterwarnings('ignore',category = UserWarning)\n",
        "warnings.filterwarnings('ignore',category = RuntimeWarning)\n",
        "warnings.filterwarnings('ignore',category = FutureWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOVSgOrdOsIm"
      },
      "source": [
        "pd.set_option('display.max_rows',100000)\n",
        "pd.set_option('display.max_columns',1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrsSe8kZOu5r"
      },
      "source": [
        "# data are xr3, yr3, xv3, yv3, xt3, yt3\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "#x_train = sc.fit_transform(xr2)\n",
        "#x_test = sc.transform(xt2)\n",
        "x_train = xr2\n",
        "x_test = xt2\n",
        "y_train = yr2\n",
        "y_test = yt2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG4eeBKeRHNO"
      },
      "source": [
        "lr = LinearRegression()\n",
        "# Linear Regression\n",
        " \n",
        "lasso = Lasso()\n",
        "# Lasso Regression\n",
        " \n",
        "ridge = Ridge()\n",
        "# Ridge Regression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5zm2_p1RIFM",
        "outputId": "922a2ebb-c567-4f7b-a929-0057d3d5d8a5"
      },
      "source": [
        "lr.fit(x_train, y_train)\n",
        "# fitting the linear regression model\n",
        "y_pred_lr = lr.predict(x_test)\n",
        "# predicting the test with linear regression model\n",
        " \n",
        "print(\"Model\\t\\t\\t RMSE \\t\\t MSE \\t\\t MAE \\t\\t R2 \\t\\t MPE\")\n",
        "print(\"\"\"LinearRegression \\t {:.2f} \\t\\t {:.2f} \\t\\t {:.2f} \\t\\t {:.2f} \\t\\t {:2f}\"\"\".format(\n",
        "            np.sqrt(mean_squared_error(y_test, y_pred_lr)),mean_squared_error(y_test, y_pred_lr),\n",
        "            mean_absolute_error(y_test, y_pred_lr), r2_score(y_test, y_pred_lr), \n",
        "            mean_absolute_percentage_error(y_test, y_pred_lr)))\n",
        " \n",
        "for i in range(20):\n",
        "    print(' test: predicted:', y_pred_lr[i], 'real data:', y_test[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model\t\t\t RMSE \t\t MSE \t\t MAE \t\t R2 \t\t MPE\n",
            "LinearRegression \t 5.26 \t\t 27.67 \t\t 4.52 \t\t 0.06 \t\t 0.163566\n",
            " test: predicted: [30.20533658] real data: [38.1]\n",
            " test: predicted: [30.23658658] real data: [32.8]\n",
            " test: predicted: [29.95533658] real data: [27.3]\n",
            " test: predicted: [29.81471158] real data: [22.4]\n",
            " test: predicted: [29.61158658] real data: [31.3]\n",
            " test: predicted: [29.75221158] real data: [32.]\n",
            " test: predicted: [29.97096158] real data: [31.2]\n",
            " test: predicted: [29.58033658] real data: [20.5]\n",
            " test: predicted: [29.68971158] real data: [27.4]\n",
            " test: predicted: [30.23658658] real data: [31.8]\n",
            " test: predicted: [29.18971158] real data: [34.1]\n",
            " test: predicted: [30.14283658] real data: [24.6]\n",
            " test: predicted: [28.93971158] real data: [18.3]\n",
            " test: predicted: [30.50221158] real data: [26.3]\n",
            " test: predicted: [29.73658658] real data: [28.1]\n",
            " test: predicted: [29.78346158] real data: [32.9]\n",
            " test: predicted: [29.56471158] real data: [22.1]\n",
            " test: predicted: [29.86158658] real data: [30.9]\n",
            " test: predicted: [30.08033658] real data: [31.2]\n",
            " test: predicted: [29.61158658] real data: [25.8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzIy5JwSk2q8"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
        "...                     hidden_layer_sizes=(5, 2), random_state=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdjWj9rrk6cM",
        "outputId": "ea9c5fff-9eca-490b-a05a-0f565303e4d5"
      },
      "source": [
        "train4, test4, validation4 = data.getFinalData('data_files/' + INPUT_TITLE + '.xlsx')\n",
        "train4, test4, validation4 = data.prepareMultipleData(train4, test4, validation4, [])\n",
        "xr4 = train4[:,0:8]\n",
        "yr4 = train4[:,8:9]\n",
        "xt4 = test4[:,0:8]\n",
        "yt4 = test4[:,8:9]\n",
        "xv4 = validation4[:,0:8]\n",
        "yv4 = validation4[:,8:9]\n",
        "utils.exceptionIfNan(train4)\n",
        "utils.exceptionIfNan(test4)\n",
        "utils.exceptionIfNan(validation4)\n",
        "print('data world ready with train:', train4.shape, 'and test:', test4.shape, 'and validation:', \n",
        "validation4.shape)\n",
        "if(SAVE_INPUT):\n",
        "  data.saveData(train4, 'train4.xlsx')\n",
        " \n",
        "model4 = utils.newSeqentialModel(8, 1)\n",
        " \n",
        "if(LOAD_WEIGHTS):\n",
        "  print('loading model weights ...')\n",
        "  output_dir = os.path.join(os.getcwd(), \"saved_wights\")\n",
        "  model4.load_weights(filepath=os.path.join(output_dir, OUTPUT_TITLE  +\"4.h5\"))\n",
        "  print('model weights loaded')\n",
        "  try:\n",
        "    model4.load_weights(filepath=os.path.join(output_dir, OUTPUT_TITLE + \"4.h5\"))\n",
        "    print('model weights loaded')\n",
        "  except OSError:\n",
        "    print('no previous weights found')\n",
        "  except ValueError:\n",
        "    print('previous weights are different from current') \n",
        " \n",
        "print('\\n\\nTrainning model world')\n",
        "model4.compile(loss='mean_squared_error', optimizer='adam', \n",
        "metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n",
        "model4.fit(xr4, yr4, epochs=NUM_EPOCHS, batch_size=64, validation_data=(xv4, yv4), \n",
        "verbose=0, callbacks=[callback.LossAndErrorPrintingCallback()])\n",
        " \n",
        "_, accuracy_test_4 = model4.evaluate(xt4, yt4)\n",
        "print('\\n\\nmodel world trained')\n",
        "print('\\nAccuracy on test data: %.2f' % (accuracy_test_4))\n",
        " \n",
        "test_predictions = np.around(model4.predict(xt4), 1)\n",
        " \n",
        "if (PRINT_PRDECTIONS):\n",
        "    for i in range(20):\n",
        "        print(' test: predicted:', test_predictions[i], 'real data:', yt4[i])\n",
        " \n",
        "if (SAVE_OUTPUT):\n",
        "    result = np.concatenate([xt4, yt4, test_predictions], axis=1)\n",
        "    data.saveData(result, 'results/' + OUTPUT_TITLE + '4.xlsx')\n",
        " \n",
        "if (SAVE_WEIGHTS):\n",
        "    print('\\nsaving model weights ...')\n",
        "    output_dir = os.path.join(os.getcwd(), \"saved_wights\")\n",
        "    model4.save_weights(filepath=os.path.join(output_dir, OUTPUT_TITLE + \"4.h5\"))\n",
        "    print('model weights saved')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading the data ...\n",
            "data loaded with size:  (1028, 9) \n",
            "\n",
            "\n",
            "data is good\n",
            "\n",
            "\n",
            "shuffling the data ...\n",
            "data shuffled\n",
            "\n",
            "\n",
            "dividing the data ...\n",
            "training data ready with size: (720, 9)\n",
            "test data ready with size: (155, 9)\n",
            "validation data ready with size: (153, 9)\n",
            "data world ready with train: (720, 9) and test: (155, 9) and validation: (153, 9)\n",
            "\n",
            "\n",
            "Trainning model world\n",
            "epoch 0 : loss : 1546.64 , MPE :  100.72. validation loss : 1646.75 , MPE :  100.43\n",
            "epoch 100 : loss :  366.75 , MPE :   69.76. validation loss :  399.76 , MPE :   69.67\n",
            "epoch 200 : loss :  275.30 , MPE :   63.13. validation loss :  284.57 , MPE :   63.13\n",
            "epoch 300 : loss :  275.30 , MPE :   62.08. validation loss :  284.71 , MPE :   62.07\n",
            "epoch 400 : loss :  275.30 , MPE :   61.55. validation loss :  284.33 , MPE :   61.55\n",
            "epoch 500 : loss :  275.37 , MPE :   61.23. validation loss :  284.48 , MPE :   61.23\n",
            "epoch 600 : loss :   70.45 , MPE :   57.78. validation loss :   89.82 , MPE :   57.75\n",
            "epoch 700 : loss :   36.59 , MPE :   52.20. validation loss :   50.71 , MPE :   52.17\n",
            "epoch 800 : loss :   31.57 , MPE :   47.68. validation loss :   52.63 , MPE :   47.66\n",
            "epoch 900 : loss :   29.12 , MPE :   44.05. validation loss :   41.62 , MPE :   44.03\n",
            "epoch 1000 : loss :   24.72 , MPE :   41.12. validation loss :   41.00 , MPE :   41.10\n",
            "epoch 1100 : loss :   26.52 , MPE :   38.66. validation loss :   39.43 , MPE :   38.65\n",
            "epoch 1200 : loss :   23.50 , MPE :   36.58. validation loss :   37.74 , MPE :   36.57\n",
            "epoch 1300 : loss :   18.50 , MPE :   34.81. validation loss :   40.03 , MPE :   34.80\n",
            "epoch 1400 : loss :   28.26 , MPE :   33.26. validation loss :   39.85 , MPE :   33.25\n",
            "epoch 1500 : loss :   20.00 , MPE :   31.89. validation loss :   37.00 , MPE :   31.89\n",
            "epoch 1600 : loss :   19.31 , MPE :   30.69. validation loss :   37.14 , MPE :   30.69\n",
            "epoch 1700 : loss :   17.88 , MPE :   29.62. validation loss :   37.28 , MPE :   29.61\n",
            "epoch 1800 : loss :   18.88 , MPE :   28.66. validation loss :   36.96 , MPE :   28.65\n",
            "epoch 1900 : loss :   16.81 , MPE :   27.79. validation loss :   35.63 , MPE :   27.79\n",
            "epoch 2000 : loss :   16.50 , MPE :   27.00. validation loss :   35.14 , MPE :   27.00\n",
            "epoch 2100 : loss :   17.30 , MPE :   26.28. validation loss :   34.98 , MPE :   26.28\n",
            "epoch 2200 : loss :   16.63 , MPE :   25.62. validation loss :   38.13 , MPE :   25.62\n",
            "epoch 2300 : loss :   14.74 , MPE :   25.00. validation loss :   35.26 , MPE :   25.00\n",
            "epoch 2400 : loss :   14.06 , MPE :   24.43. validation loss :   35.48 , MPE :   24.43\n",
            "epoch 2500 : loss :   12.71 , MPE :   23.91. validation loss :   34.23 , MPE :   23.90\n",
            "epoch 2600 : loss :   14.97 , MPE :   23.41. validation loss :   36.55 , MPE :   23.41\n",
            "epoch 2700 : loss :   14.93 , MPE :   22.95. validation loss :   34.70 , MPE :   22.95\n",
            "epoch 2800 : loss :   14.34 , MPE :   22.52. validation loss :   33.19 , MPE :   22.52\n",
            "epoch 2900 : loss :   13.26 , MPE :   22.11. validation loss :   33.09 , MPE :   22.11\n",
            "epoch 3000 : loss :   13.12 , MPE :   21.73. validation loss :   34.55 , MPE :   21.73\n",
            "epoch 3100 : loss :   13.72 , MPE :   21.37. validation loss :   34.38 , MPE :   21.37\n",
            "epoch 3200 : loss :   11.29 , MPE :   21.03. validation loss :   34.96 , MPE :   21.03\n",
            "epoch 3300 : loss :   13.60 , MPE :   20.71. validation loss :   36.73 , MPE :   20.71\n",
            "epoch 3400 : loss :   14.20 , MPE :   20.40. validation loss :   37.11 , MPE :   20.40\n",
            "epoch 3500 : loss :   10.93 , MPE :   20.10. validation loss :   37.19 , MPE :   20.10\n",
            "epoch 3600 : loss :   13.27 , MPE :   19.82. validation loss :   35.19 , MPE :   19.82\n",
            "epoch 3700 : loss :   10.53 , MPE :   19.56. validation loss :   39.89 , MPE :   19.56\n",
            "epoch 3800 : loss :   10.90 , MPE :   19.31. validation loss :   35.62 , MPE :   19.31\n",
            "epoch 3900 : loss :   10.48 , MPE :   19.06. validation loss :   40.31 , MPE :   19.06\n",
            "epoch 4000 : loss :   11.95 , MPE :   18.84. validation loss :   36.37 , MPE :   18.84\n",
            "epoch 4100 : loss :    8.74 , MPE :   18.62. validation loss :   39.10 , MPE :   18.62\n",
            "epoch 4200 : loss :    9.64 , MPE :   18.40. validation loss :   42.91 , MPE :   18.40\n",
            "epoch 4300 : loss :   10.14 , MPE :   18.20. validation loss :   35.76 , MPE :   18.20\n",
            "epoch 4400 : loss :    9.36 , MPE :   18.01. validation loss :   37.20 , MPE :   18.01\n",
            "epoch 4500 : loss :    8.96 , MPE :   17.82. validation loss :   34.50 , MPE :   17.82\n",
            "epoch 4600 : loss :    8.99 , MPE :   17.64. validation loss :   36.60 , MPE :   17.64\n",
            "epoch 4700 : loss :    9.90 , MPE :   17.47. validation loss :   34.36 , MPE :   17.46\n",
            "epoch 4800 : loss :    9.56 , MPE :   17.30. validation loss :   36.02 , MPE :   17.30\n",
            "epoch 4900 : loss :    9.86 , MPE :   17.14. validation loss :   38.09 , MPE :   17.14\n",
            "epoch 5000 : loss :    8.24 , MPE :   16.99. validation loss :   37.45 , MPE :   16.99\n",
            "epoch 5100 : loss :    9.05 , MPE :   16.84. validation loss :   37.39 , MPE :   16.83\n",
            "epoch 5200 : loss :    9.43 , MPE :   16.69. validation loss :   34.48 , MPE :   16.69\n",
            "epoch 5300 : loss :    8.19 , MPE :   16.55. validation loss :   34.89 , MPE :   16.55\n",
            "epoch 5400 : loss :    9.47 , MPE :   16.42. validation loss :   37.98 , MPE :   16.42\n",
            "epoch 5500 : loss :    8.80 , MPE :   16.29. validation loss :   37.76 , MPE :   16.28\n",
            "epoch 5600 : loss :   11.41 , MPE :   16.16. validation loss :   33.84 , MPE :   16.16\n",
            "epoch 5700 : loss :   12.05 , MPE :   16.03. validation loss :   44.28 , MPE :   16.03\n",
            "epoch 5800 : loss :    9.30 , MPE :   15.92. validation loss :   39.46 , MPE :   15.92\n",
            "epoch 5900 : loss :    8.02 , MPE :   15.80. validation loss :   37.41 , MPE :   15.80\n",
            "epoch 6000 : loss :    8.34 , MPE :   15.69. validation loss :   37.03 , MPE :   15.69\n",
            "epoch 6100 : loss :    7.58 , MPE :   15.58. validation loss :   39.31 , MPE :   15.58\n",
            "epoch 6200 : loss :    8.38 , MPE :   15.47. validation loss :   38.27 , MPE :   15.47\n",
            "epoch 6300 : loss :    8.66 , MPE :   15.37. validation loss :   39.91 , MPE :   15.37\n",
            "epoch 6400 : loss :    8.87 , MPE :   15.27. validation loss :   41.65 , MPE :   15.27\n",
            "epoch 6500 : loss :    6.69 , MPE :   15.17. validation loss :   36.69 , MPE :   15.17\n",
            "epoch 6600 : loss :    8.05 , MPE :   15.08. validation loss :   36.26 , MPE :   15.08\n",
            "epoch 6700 : loss :    8.26 , MPE :   14.98. validation loss :   39.94 , MPE :   14.98\n",
            "epoch 6800 : loss :    6.89 , MPE :   14.90. validation loss :   39.13 , MPE :   14.90\n",
            "epoch 6900 : loss :    7.96 , MPE :   14.81. validation loss :   38.36 , MPE :   14.81\n",
            "epoch 7000 : loss :    8.74 , MPE :   14.72. validation loss :   36.62 , MPE :   14.72\n",
            "epoch 7100 : loss :    8.34 , MPE :   14.64. validation loss :   39.21 , MPE :   14.64\n",
            "epoch 7200 : loss :    8.61 , MPE :   14.56. validation loss :   37.63 , MPE :   14.56\n",
            "epoch 7300 : loss :    7.27 , MPE :   14.48. validation loss :   37.90 , MPE :   14.48\n",
            "epoch 7400 : loss :    7.92 , MPE :   14.40. validation loss :   38.07 , MPE :   14.40\n",
            "epoch 7500 : loss :    6.96 , MPE :   14.33. validation loss :   37.08 , MPE :   14.33\n",
            "epoch 7600 : loss :    8.54 , MPE :   14.25. validation loss :   39.37 , MPE :   14.25\n",
            "epoch 7700 : loss :    9.99 , MPE :   14.18. validation loss :   39.30 , MPE :   14.18\n",
            "epoch 7800 : loss :    9.81 , MPE :   14.11. validation loss :   37.50 , MPE :   14.11\n",
            "epoch 7900 : loss :    7.86 , MPE :   14.04. validation loss :   36.21 , MPE :   14.04\n",
            "epoch 8000 : loss :    7.03 , MPE :   13.98. validation loss :   40.88 , MPE :   13.97\n",
            "epoch 8100 : loss :    8.69 , MPE :   13.91. validation loss :   41.26 , MPE :   13.91\n",
            "epoch 8200 : loss :    7.70 , MPE :   13.84. validation loss :   36.05 , MPE :   13.84\n",
            "epoch 8300 : loss :    6.86 , MPE :   13.78. validation loss :   36.66 , MPE :   13.78\n",
            "epoch 8400 : loss :    7.81 , MPE :   13.72. validation loss :   37.25 , MPE :   13.72\n",
            "epoch 8500 : loss :    9.30 , MPE :   13.66. validation loss :   37.35 , MPE :   13.66\n",
            "epoch 8600 : loss :    7.41 , MPE :   13.60. validation loss :   36.50 , MPE :   13.60\n",
            "epoch 8700 : loss :    6.70 , MPE :   13.54. validation loss :   35.80 , MPE :   13.54\n",
            "epoch 8800 : loss :    8.41 , MPE :   13.48. validation loss :   37.75 , MPE :   13.48\n",
            "epoch 8900 : loss :    8.82 , MPE :   13.43. validation loss :   37.75 , MPE :   13.42\n",
            "epoch 9000 : loss :    6.61 , MPE :   13.37. validation loss :   37.34 , MPE :   13.37\n",
            "epoch 9100 : loss :    6.44 , MPE :   13.32. validation loss :   35.64 , MPE :   13.32\n",
            "epoch 9200 : loss :    8.00 , MPE :   13.26. validation loss :   38.44 , MPE :   13.26\n",
            "epoch 9300 : loss :    7.24 , MPE :   13.21. validation loss :   36.84 , MPE :   13.21\n",
            "epoch 9400 : loss :    8.57 , MPE :   13.16. validation loss :   37.14 , MPE :   13.16\n",
            "epoch 9500 : loss :    7.64 , MPE :   13.11. validation loss :   36.14 , MPE :   13.11\n",
            "epoch 9600 : loss :    7.29 , MPE :   13.06. validation loss :   33.47 , MPE :   13.06\n",
            "epoch 9700 : loss :    7.44 , MPE :   13.01. validation loss :   37.00 , MPE :   13.01\n",
            "epoch 9800 : loss :   12.48 , MPE :   12.97. validation loss :   39.59 , MPE :   12.97\n",
            "epoch 9900 : loss :    6.78 , MPE :   12.92. validation loss :   34.62 , MPE :   12.92\n",
            "epoch 10000 : loss :   11.63 , MPE :   12.88. validation loss :   41.35 , MPE :   12.88\n",
            "epoch 10100 : loss :    7.56 , MPE :   12.83. validation loss :   35.15 , MPE :   12.83\n",
            "epoch 10200 : loss :    7.51 , MPE :   12.79. validation loss :   38.86 , MPE :   12.79\n",
            "epoch 10300 : loss :    6.12 , MPE :   12.74. validation loss :   36.36 , MPE :   12.74\n",
            "epoch 10400 : loss :    6.74 , MPE :   12.70. validation loss :   36.72 , MPE :   12.70\n",
            "epoch 10500 : loss :    6.46 , MPE :   12.66. validation loss :   35.92 , MPE :   12.66\n",
            "epoch 10600 : loss :    7.49 , MPE :   12.62. validation loss :   35.83 , MPE :   12.62\n",
            "epoch 10700 : loss :    6.31 , MPE :   12.58. validation loss :   34.84 , MPE :   12.58\n",
            "epoch 10800 : loss :    6.18 , MPE :   12.54. validation loss :   37.85 , MPE :   12.54\n",
            "epoch 10900 : loss :    5.92 , MPE :   12.50. validation loss :   38.41 , MPE :   12.50\n",
            "epoch 11000 : loss :    6.56 , MPE :   12.46. validation loss :   36.38 , MPE :   12.46\n",
            "epoch 11100 : loss :    6.79 , MPE :   12.42. validation loss :   37.99 , MPE :   12.42\n",
            "epoch 11200 : loss :    6.93 , MPE :   12.38. validation loss :   40.87 , MPE :   12.38\n",
            "epoch 11300 : loss :    6.25 , MPE :   12.35. validation loss :   33.57 , MPE :   12.35\n",
            "epoch 11400 : loss :    7.30 , MPE :   12.31. validation loss :   36.97 , MPE :   12.31\n",
            "epoch 11500 : loss :    6.34 , MPE :   12.28. validation loss :   35.49 , MPE :   12.28\n",
            "epoch 11600 : loss :    6.00 , MPE :   12.24. validation loss :   35.26 , MPE :   12.24\n",
            "epoch 11700 : loss :    6.66 , MPE :   12.21. validation loss :   35.62 , MPE :   12.20\n",
            "epoch 11800 : loss :    7.21 , MPE :   12.17. validation loss :   38.05 , MPE :   12.17\n",
            "epoch 11900 : loss :    7.29 , MPE :   12.14. validation loss :   35.91 , MPE :   12.14\n",
            "epoch 12000 : loss :    6.58 , MPE :   12.10. validation loss :   35.29 , MPE :   12.10\n",
            "epoch 12100 : loss :    6.43 , MPE :   12.07. validation loss :   37.27 , MPE :   12.07\n",
            "epoch 12200 : loss :    6.99 , MPE :   12.04. validation loss :   38.31 , MPE :   12.04\n",
            "epoch 12300 : loss :    6.82 , MPE :   12.01. validation loss :   37.76 , MPE :   12.00\n",
            "epoch 12400 : loss :    7.16 , MPE :   11.97. validation loss :   37.22 , MPE :   11.97\n",
            "epoch 12500 : loss :    8.38 , MPE :   11.95. validation loss :   34.62 , MPE :   11.95\n",
            "epoch 12600 : loss :    6.73 , MPE :   11.92. validation loss :   37.36 , MPE :   11.92\n",
            "epoch 12700 : loss :    5.66 , MPE :   11.88. validation loss :   35.81 , MPE :   11.88\n",
            "epoch 12800 : loss :    6.39 , MPE :   11.86. validation loss :   37.19 , MPE :   11.86\n",
            "epoch 12900 : loss :    7.05 , MPE :   11.83. validation loss :   36.14 , MPE :   11.83\n",
            "epoch 13000 : loss :    6.81 , MPE :   11.80. validation loss :   36.97 , MPE :   11.80\n",
            "epoch 13100 : loss :    5.41 , MPE :   11.77. validation loss :   36.38 , MPE :   11.77\n",
            "epoch 13200 : loss :    7.67 , MPE :   11.74. validation loss :   38.17 , MPE :   11.74\n",
            "epoch 13300 : loss :    6.46 , MPE :   11.71. validation loss :   35.88 , MPE :   11.71\n",
            "epoch 13400 : loss :    6.71 , MPE :   11.69. validation loss :   36.48 , MPE :   11.69\n",
            "epoch 13500 : loss :    7.34 , MPE :   11.66. validation loss :   34.68 , MPE :   11.66\n",
            "epoch 13600 : loss :    5.51 , MPE :   11.63. validation loss :   36.52 , MPE :   11.63\n",
            "epoch 13700 : loss :    6.15 , MPE :   11.60. validation loss :   35.33 , MPE :   11.60\n",
            "epoch 13800 : loss :    5.79 , MPE :   11.58. validation loss :   33.42 , MPE :   11.58\n",
            "epoch 13900 : loss :    5.66 , MPE :   11.55. validation loss :   36.89 , MPE :   11.55\n",
            "epoch 14000 : loss :    6.01 , MPE :   11.53. validation loss :   34.35 , MPE :   11.53\n",
            "epoch 14100 : loss :    6.26 , MPE :   11.50. validation loss :   33.40 , MPE :   11.50\n",
            "epoch 14200 : loss :    7.52 , MPE :   11.47. validation loss :   34.89 , MPE :   11.47\n",
            "epoch 14300 : loss :    6.38 , MPE :   11.45. validation loss :   33.36 , MPE :   11.45\n",
            "epoch 14400 : loss :    6.45 , MPE :   11.43. validation loss :   34.54 , MPE :   11.43\n",
            "epoch 14500 : loss :    6.01 , MPE :   11.40. validation loss :   36.11 , MPE :   11.40\n",
            "epoch 14600 : loss :    7.66 , MPE :   11.38. validation loss :   34.44 , MPE :   11.38\n",
            "epoch 14700 : loss :    7.58 , MPE :   11.35. validation loss :   34.64 , MPE :   11.35\n",
            "epoch 14800 : loss :    5.92 , MPE :   11.33. validation loss :   33.59 , MPE :   11.33\n",
            "epoch 14900 : loss :    6.55 , MPE :   11.31. validation loss :   37.92 , MPE :   11.31\n",
            "epoch 15000 : loss :    8.65 , MPE :   11.28. validation loss :   40.54 , MPE :   11.28\n",
            "epoch 15100 : loss :    7.45 , MPE :   11.26. validation loss :   40.09 , MPE :   11.26\n",
            "epoch 15200 : loss :    6.39 , MPE :   11.24. validation loss :   37.51 , MPE :   11.24\n",
            "epoch 15300 : loss :    6.26 , MPE :   11.22. validation loss :   36.09 , MPE :   11.22\n",
            "epoch 15400 : loss :    7.08 , MPE :   11.20. validation loss :   37.21 , MPE :   11.20\n",
            "epoch 15500 : loss :    6.54 , MPE :   11.17. validation loss :   35.90 , MPE :   11.17\n",
            "epoch 15600 : loss :    7.28 , MPE :   11.15. validation loss :   31.65 , MPE :   11.15\n",
            "epoch 15700 : loss :    6.21 , MPE :   11.13. validation loss :   32.85 , MPE :   11.13\n",
            "epoch 15800 : loss :    5.62 , MPE :   11.11. validation loss :   37.13 , MPE :   11.11\n",
            "epoch 15900 : loss :    6.77 , MPE :   11.09. validation loss :   37.09 , MPE :   11.09\n",
            "epoch 16000 : loss :    6.91 , MPE :   11.07. validation loss :   38.36 , MPE :   11.07\n",
            "epoch 16100 : loss :    4.83 , MPE :   11.05. validation loss :   38.54 , MPE :   11.05\n",
            "epoch 16200 : loss :    9.09 , MPE :   11.03. validation loss :   29.50 , MPE :   11.03\n",
            "epoch 16300 : loss :    5.86 , MPE :   11.01. validation loss :   34.19 , MPE :   11.01\n",
            "epoch 16400 : loss :    5.25 , MPE :   10.99. validation loss :   34.70 , MPE :   10.99\n",
            "epoch 16500 : loss :    6.30 , MPE :   10.97. validation loss :   34.18 , MPE :   10.97\n",
            "epoch 16600 : loss :    6.57 , MPE :   10.95. validation loss :   35.12 , MPE :   10.95\n",
            "epoch 16700 : loss :    6.35 , MPE :   10.93. validation loss :   35.44 , MPE :   10.93\n",
            "epoch 16800 : loss :    8.22 , MPE :   10.91. validation loss :   38.29 , MPE :   10.91\n",
            "epoch 16900 : loss :    5.99 , MPE :   10.89. validation loss :   35.02 , MPE :   10.89\n",
            "epoch 17000 : loss :    6.86 , MPE :   10.88. validation loss :   37.73 , MPE :   10.87\n",
            "epoch 17100 : loss :    5.77 , MPE :   10.86. validation loss :   34.29 , MPE :   10.86\n",
            "epoch 17200 : loss :    6.42 , MPE :   10.84. validation loss :   36.88 , MPE :   10.84\n",
            "epoch 17300 : loss :   12.70 , MPE :   10.83. validation loss :   39.66 , MPE :   10.83\n",
            "epoch 17400 : loss :    8.48 , MPE :   10.82. validation loss :   34.66 , MPE :   10.82\n",
            "epoch 17500 : loss :    5.69 , MPE :   10.81. validation loss :   32.06 , MPE :   10.81\n",
            "epoch 17600 : loss :    6.39 , MPE :   10.79. validation loss :   36.76 , MPE :   10.79\n",
            "epoch 17700 : loss :    5.89 , MPE :   10.77. validation loss :   32.40 , MPE :   10.77\n",
            "epoch 17800 : loss :    6.21 , MPE :   10.76. validation loss :   36.16 , MPE :   10.76\n",
            "epoch 17900 : loss :    6.44 , MPE :   10.74. validation loss :   38.77 , MPE :   10.74\n",
            "epoch 18000 : loss :    5.81 , MPE :   10.72. validation loss :   34.24 , MPE :   10.72\n",
            "epoch 18100 : loss :    5.59 , MPE :   10.71. validation loss :   37.03 , MPE :   10.71\n",
            "epoch 18200 : loss :    5.49 , MPE :   10.69. validation loss :   34.94 , MPE :   10.69\n",
            "epoch 18300 : loss :    6.32 , MPE :   10.67. validation loss :   36.29 , MPE :   10.67\n",
            "epoch 18400 : loss :   10.72 , MPE :   10.66. validation loss :   38.08 , MPE :   10.66\n",
            "epoch 18500 : loss :    5.49 , MPE :   10.64. validation loss :   38.51 , MPE :   10.64\n",
            "epoch 18600 : loss :    5.73 , MPE :   10.63. validation loss :   35.91 , MPE :   10.62\n",
            "epoch 18700 : loss :    5.08 , MPE :   10.61. validation loss :   34.82 , MPE :   10.61\n",
            "epoch 18800 : loss :    6.76 , MPE :   10.59. validation loss :   37.22 , MPE :   10.59\n",
            "epoch 18900 : loss :    5.56 , MPE :   10.58. validation loss :   34.95 , MPE :   10.58\n",
            "epoch 19000 : loss :    4.62 , MPE :   10.56. validation loss :   36.37 , MPE :   10.56\n",
            "epoch 19100 : loss :    5.77 , MPE :   10.54. validation loss :   39.39 , MPE :   10.54\n",
            "epoch 19200 : loss :    8.61 , MPE :   10.53. validation loss :   36.12 , MPE :   10.53\n",
            "epoch 19300 : loss :    5.11 , MPE :   10.51. validation loss :   35.14 , MPE :   10.51\n",
            "epoch 19400 : loss :    5.40 , MPE :   10.50. validation loss :   32.60 , MPE :   10.50\n",
            "epoch 19500 : loss :    6.73 , MPE :   10.48. validation loss :   35.01 , MPE :   10.48\n",
            "epoch 19600 : loss :    7.09 , MPE :   10.47. validation loss :   34.87 , MPE :   10.47\n",
            "epoch 19700 : loss :    5.01 , MPE :   10.45. validation loss :   32.04 , MPE :   10.45\n",
            "epoch 19800 : loss :    6.47 , MPE :   10.44. validation loss :   34.85 , MPE :   10.44\n",
            "epoch 19900 : loss :    6.24 , MPE :   10.42. validation loss :   36.41 , MPE :   10.42\n",
            "epoch 20000 : loss :    5.38 , MPE :   10.41. validation loss :   34.48 , MPE :   10.41\n",
            "epoch 20100 : loss :    6.60 , MPE :   10.39. validation loss :   32.72 , MPE :   10.39\n",
            "epoch 20200 : loss :    6.31 , MPE :   10.38. validation loss :   38.11 , MPE :   10.38\n",
            "epoch 20300 : loss :    7.21 , MPE :   10.37. validation loss :   35.66 , MPE :   10.37\n",
            "epoch 20400 : loss :    6.18 , MPE :   10.35. validation loss :   34.08 , MPE :   10.35\n",
            "epoch 20500 : loss :    4.96 , MPE :   10.34. validation loss :   36.25 , MPE :   10.34\n",
            "epoch 20600 : loss :    5.26 , MPE :   10.32. validation loss :   31.69 , MPE :   10.32\n",
            "epoch 20700 : loss :    5.46 , MPE :   10.31. validation loss :   32.82 , MPE :   10.31\n",
            "epoch 20800 : loss :    6.66 , MPE :   10.30. validation loss :   35.35 , MPE :   10.30\n",
            "epoch 20900 : loss :    5.74 , MPE :   10.28. validation loss :   34.01 , MPE :   10.28\n",
            "epoch 21000 : loss :    4.56 , MPE :   10.27. validation loss :   33.66 , MPE :   10.27\n",
            "epoch 21100 : loss :    5.29 , MPE :   10.25. validation loss :   35.36 , MPE :   10.25\n",
            "epoch 21200 : loss :    5.83 , MPE :   10.24. validation loss :   33.34 , MPE :   10.24\n",
            "epoch 21300 : loss :    4.69 , MPE :   10.23. validation loss :   32.29 , MPE :   10.23\n",
            "epoch 21400 : loss :    6.39 , MPE :   10.22. validation loss :   32.72 , MPE :   10.22\n",
            "epoch 21500 : loss :    4.88 , MPE :   10.20. validation loss :   34.38 , MPE :   10.20\n",
            "epoch 21600 : loss :    6.43 , MPE :   10.19. validation loss :   32.79 , MPE :   10.19\n",
            "epoch 21700 : loss :    5.29 , MPE :   10.18. validation loss :   32.15 , MPE :   10.18\n",
            "epoch 21800 : loss :    4.71 , MPE :   10.16. validation loss :   33.06 , MPE :   10.16\n",
            "epoch 21900 : loss :    6.97 , MPE :   10.15. validation loss :   36.95 , MPE :   10.15\n",
            "epoch 22000 : loss :    4.79 , MPE :   10.14. validation loss :   32.53 , MPE :   10.14\n",
            "epoch 22100 : loss :    8.62 , MPE :   10.13. validation loss :   33.89 , MPE :   10.13\n",
            "epoch 22200 : loss :    5.42 , MPE :   10.12. validation loss :   35.39 , MPE :   10.12\n",
            "epoch 22300 : loss :    4.51 , MPE :   10.10. validation loss :   36.20 , MPE :   10.10\n",
            "epoch 22400 : loss :    5.19 , MPE :   10.09. validation loss :   38.82 , MPE :   10.09\n",
            "epoch 22500 : loss :    5.45 , MPE :   10.08. validation loss :   34.40 , MPE :   10.08\n",
            "epoch 22600 : loss :    6.26 , MPE :   10.07. validation loss :   32.50 , MPE :   10.07\n",
            "epoch 22700 : loss :    4.92 , MPE :   10.05. validation loss :   35.31 , MPE :   10.05\n",
            "epoch 22800 : loss :    6.44 , MPE :   10.04. validation loss :   33.18 , MPE :   10.04\n",
            "epoch 22900 : loss :   13.24 , MPE :   10.04. validation loss :   36.60 , MPE :   10.04\n",
            "epoch 23000 : loss :    6.13 , MPE :   10.03. validation loss :   37.54 , MPE :   10.03\n",
            "epoch 23100 : loss :    5.96 , MPE :   10.02. validation loss :   36.53 , MPE :   10.02\n",
            "epoch 23200 : loss :    4.99 , MPE :   10.01. validation loss :   35.13 , MPE :   10.01\n",
            "epoch 23300 : loss :    6.03 , MPE :    9.99. validation loss :   33.31 , MPE :    9.99\n",
            "epoch 23400 : loss :    5.63 , MPE :    9.98. validation loss :   40.00 , MPE :    9.98\n",
            "epoch 23500 : loss :    8.40 , MPE :    9.97. validation loss :   33.78 , MPE :    9.97\n",
            "epoch 23600 : loss :    7.00 , MPE :    9.96. validation loss :   32.34 , MPE :    9.96\n",
            "epoch 23700 : loss :    5.26 , MPE :    9.95. validation loss :   33.53 , MPE :    9.95\n",
            "epoch 23800 : loss :    6.32 , MPE :    9.94. validation loss :   32.77 , MPE :    9.94\n",
            "epoch 23900 : loss :    5.52 , MPE :    9.93. validation loss :   37.41 , MPE :    9.93\n",
            "epoch 24000 : loss :    5.61 , MPE :    9.92. validation loss :   39.59 , MPE :    9.92\n",
            "epoch 24100 : loss :    5.62 , MPE :    9.91. validation loss :   36.66 , MPE :    9.91\n",
            "epoch 24200 : loss :    6.09 , MPE :    9.89. validation loss :   35.72 , MPE :    9.89\n",
            "epoch 24300 : loss :    5.89 , MPE :    9.88. validation loss :   33.96 , MPE :    9.88\n",
            "epoch 24400 : loss :    5.02 , MPE :    9.87. validation loss :   38.68 , MPE :    9.87\n",
            "epoch 24500 : loss :    5.86 , MPE :    9.86. validation loss :   35.38 , MPE :    9.86\n",
            "epoch 24600 : loss :    4.63 , MPE :    9.85. validation loss :   34.15 , MPE :    9.85\n",
            "epoch 24700 : loss :    4.87 , MPE :    9.84. validation loss :   32.96 , MPE :    9.84\n",
            "epoch 24800 : loss :    4.95 , MPE :    9.83. validation loss :   33.94 , MPE :    9.83\n",
            "epoch 24900 : loss :    6.49 , MPE :    9.82. validation loss :   34.56 , MPE :    9.82\n",
            "epoch 25000 : loss :    5.28 , MPE :    9.81. validation loss :   36.40 , MPE :    9.81\n",
            "epoch 25100 : loss :    4.85 , MPE :    9.80. validation loss :   36.80 , MPE :    9.80\n",
            "epoch 25200 : loss :    6.98 , MPE :    9.79. validation loss :   35.53 , MPE :    9.79\n",
            "epoch 25300 : loss :    4.66 , MPE :    9.78. validation loss :   34.83 , MPE :    9.78\n",
            "epoch 25400 : loss :    6.69 , MPE :    9.77. validation loss :   37.84 , MPE :    9.77\n",
            "epoch 25500 : loss :    4.60 , MPE :    9.76. validation loss :   38.72 , MPE :    9.76\n",
            "epoch 25600 : loss :    9.34 , MPE :    9.75. validation loss :   36.43 , MPE :    9.75\n",
            "epoch 25700 : loss :    4.92 , MPE :    9.74. validation loss :   37.49 , MPE :    9.74\n",
            "epoch 25800 : loss :    6.18 , MPE :    9.73. validation loss :   37.08 , MPE :    9.73\n",
            "epoch 25900 : loss :    6.33 , MPE :    9.73. validation loss :   39.19 , MPE :    9.73\n",
            "epoch 26000 : loss :    6.74 , MPE :    9.72. validation loss :   44.17 , MPE :    9.72\n",
            "epoch 26100 : loss :    4.68 , MPE :    9.71. validation loss :   37.79 , MPE :    9.71\n",
            "epoch 26200 : loss :    4.60 , MPE :    9.70. validation loss :   36.88 , MPE :    9.70\n",
            "epoch 26300 : loss :    6.36 , MPE :    9.69. validation loss :   40.36 , MPE :    9.69\n",
            "epoch 26400 : loss :    5.46 , MPE :    9.68. validation loss :   31.97 , MPE :    9.68\n",
            "epoch 26500 : loss :    5.07 , MPE :    9.67. validation loss :   35.45 , MPE :    9.67\n",
            "epoch 26600 : loss :   11.98 , MPE :    9.66. validation loss :   38.65 , MPE :    9.66\n",
            "epoch 26700 : loss :    9.44 , MPE :    9.65. validation loss :   40.54 , MPE :    9.65\n",
            "epoch 26800 : loss :    4.62 , MPE :    9.64. validation loss :   34.40 , MPE :    9.64\n",
            "epoch 26900 : loss :    7.80 , MPE :    9.64. validation loss :   32.46 , MPE :    9.64\n",
            "epoch 27000 : loss :    5.45 , MPE :    9.63. validation loss :   35.13 , MPE :    9.63\n",
            "epoch 27100 : loss :    4.56 , MPE :    9.62. validation loss :   35.31 , MPE :    9.62\n",
            "epoch 27200 : loss :    5.12 , MPE :    9.61. validation loss :   34.71 , MPE :    9.61\n",
            "epoch 27300 : loss :    5.74 , MPE :    9.60. validation loss :   37.19 , MPE :    9.60\n",
            "epoch 27400 : loss :    5.85 , MPE :    9.59. validation loss :   35.88 , MPE :    9.59\n",
            "epoch 27500 : loss :    5.13 , MPE :    9.58. validation loss :   36.21 , MPE :    9.58\n",
            "epoch 27600 : loss :    5.39 , MPE :    9.58. validation loss :   34.52 , MPE :    9.58\n",
            "epoch 27700 : loss :    5.38 , MPE :    9.57. validation loss :   33.29 , MPE :    9.57\n",
            "epoch 27800 : loss :    4.40 , MPE :    9.56. validation loss :   37.44 , MPE :    9.56\n",
            "epoch 27900 : loss :    5.13 , MPE :    9.55. validation loss :   36.75 , MPE :    9.55\n",
            "epoch 28000 : loss :    4.88 , MPE :    9.54. validation loss :   35.91 , MPE :    9.54\n",
            "epoch 28100 : loss :   11.04 , MPE :    9.53. validation loss :   40.52 , MPE :    9.53\n",
            "epoch 28200 : loss :    4.24 , MPE :    9.53. validation loss :   34.76 , MPE :    9.53\n",
            "epoch 28300 : loss :    4.82 , MPE :    9.52. validation loss :   35.67 , MPE :    9.52\n",
            "epoch 28400 : loss :   10.63 , MPE :    9.51. validation loss :   32.06 , MPE :    9.51\n",
            "epoch 28500 : loss :    8.36 , MPE :    9.50. validation loss :   39.61 , MPE :    9.50\n",
            "epoch 28600 : loss :    8.11 , MPE :    9.49. validation loss :   34.63 , MPE :    9.49\n",
            "epoch 28700 : loss :    5.06 , MPE :    9.49. validation loss :   36.58 , MPE :    9.49\n",
            "epoch 28800 : loss :    4.96 , MPE :    9.48. validation loss :   37.84 , MPE :    9.48\n",
            "epoch 28900 : loss :    5.38 , MPE :    9.47. validation loss :   43.13 , MPE :    9.47\n",
            "epoch 29000 : loss :    4.69 , MPE :    9.46. validation loss :   38.05 , MPE :    9.46\n",
            "epoch 29100 : loss :    8.95 , MPE :    9.45. validation loss :   34.35 , MPE :    9.45\n",
            "epoch 29200 : loss :    5.14 , MPE :    9.45. validation loss :   34.83 , MPE :    9.45\n",
            "epoch 29300 : loss :    5.18 , MPE :    9.44. validation loss :   34.40 , MPE :    9.44\n",
            "epoch 29400 : loss :    4.53 , MPE :    9.43. validation loss :   37.17 , MPE :    9.43\n",
            "epoch 29500 : loss :    7.93 , MPE :    9.42. validation loss :   36.42 , MPE :    9.42\n",
            "epoch 29600 : loss :    6.48 , MPE :    9.42. validation loss :   38.66 , MPE :    9.42\n",
            "epoch 29700 : loss :    9.81 , MPE :    9.41. validation loss :   37.62 , MPE :    9.41\n",
            "epoch 29800 : loss :    6.09 , MPE :    9.40. validation loss :   35.14 , MPE :    9.40\n",
            "epoch 29900 : loss :    9.90 , MPE :    9.39. validation loss :   37.74 , MPE :    9.39\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 21.7322 - mean_absolute_percentage_error: 9.3885\n",
            "\n",
            "\n",
            "model world trained\n",
            "\n",
            "Accuracy on test data: 9.39\n",
            " test: predicted: [53.] real data: [56.3991368]\n",
            " test: predicted: [40.2] real data: [33.39821744]\n",
            " test: predicted: [11.7] real data: [9.73815902]\n",
            " test: predicted: [13.7] real data: [11.57630204]\n",
            " test: predicted: [53.] real data: [52.44154456]\n",
            " test: predicted: [38.9] real data: [33.0431373]\n",
            " test: predicted: [28.3] real data: [30.43967592]\n",
            " test: predicted: [16.8] real data: [9.69472204]\n",
            " test: predicted: [35.9] real data: [34.48758952]\n",
            " test: predicted: [28.2] real data: [32.10269204]\n",
            " test: predicted: [38.1] real data: [37.42475728]\n",
            " test: predicted: [46.] real data: [44.13335876]\n",
            " test: predicted: [15.5] real data: [17.24379476]\n",
            " test: predicted: [35.5] real data: [31.38150014]\n",
            " test: predicted: [15.2] real data: [13.12072828]\n",
            " test: predicted: [10.7] real data: [7.50701469]\n",
            " test: predicted: [46.1] real data: [39.37597436]\n",
            " test: predicted: [76.6] real data: [77.29715436]\n",
            " test: predicted: [12.6] real data: [11.85209244]\n",
            " test: predicted: [26.] real data: [32.32952964]\n",
            "saved data to excel file \n",
            "path: results/world_data4.xlsx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3iemyD50wCa"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yYi0DS30wWT"
      },
      "source": [
        "**Testing model 7 days with grade 25**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiwsyL6d02Nz",
        "outputId": "3ff2e088-6a41-4d00-f59b-95200859ec70"
      },
      "source": [
        "train2, test2, validation2 = data.getFinalData('data_files/omer_data.xlsx')\n",
        "train2, test2, validation2 = data.prepareMultipleData(train2, test2, validation2, [13, 15, 16])\n",
        "CURRENT_GRADE = 30\n",
        " \n",
        "#train2 = train2[train2[:,5] == CURRENT_GRADE]\n",
        "#train2 = np.delete(train2, [5, 9, 10], 1)\n",
        "#test2 = test2[test2[:,5] == CURRENT_GRADE]\n",
        "#test2 = np.delete(test2,[5, 9, 10], 1)\n",
        "#validation2 = validation2[validation2[:,5] == CURRENT_GRADE]\n",
        "#validation2 = np.delete(validation2, [5, 9, 10], 1)\n",
        " \n",
        "#train2 = train2[train2[:,14] >= 60]\n",
        "#train2 = np.delete(train2, [0, 1], 1)\n",
        "#test2 = test2[test2[:,14] >= 69]\n",
        "#test2 = np.delete(test2, [0, 1], 1)\n",
        "#validation2 = validation2[validation2[:,14] >= 60]\n",
        "#validation2 = np.delete(validation2, [0, 1], 1)\n",
        " \n",
        "tmp1 = train2.shape[1] - 1\n",
        "tmp2 = train2.shape[1]\n",
        " \n",
        "xr2 = train2[:,0:tmp1]\n",
        "yr2 = train2[:,tmp1:tmp2]\n",
        "xt2 = test2[:,0:tmp1]\n",
        "yt2 = test2[:,tmp1:tmp2]\n",
        "xv2 = validation2[:,0:tmp1]\n",
        "yv2 = validation2[:,tmp1:tmp2]\n",
        " \n",
        "utils.exceptionIfNan(train2)\n",
        "utils.exceptionIfNan(test2)\n",
        "utils.exceptionIfNan(validation2)\n",
        "print('data 2 ready with train:', train2.shape, 'and test:', test2.shape, 'and validation:', \n",
        "validation2.shape)\n",
        "if(SAVE_INPUT):\n",
        "    data.saveData(train2, 'train2.xlsx')\n",
        " \n",
        "model2 = utils.newSeqentialModel(xr2.shape[1], yr2.shape[1])\n",
        " \n",
        "if(LOAD_WEIGHTS):\n",
        "    print('loading model weights ...')\n",
        "    output_dir = os.path.join(os.getcwd(), \"saved_wights\")\n",
        "    try:\n",
        "      model2.load_weights(filepath=os.path.join(output_dir, OUTPUT_TITLE + \"2.h5\"))\n",
        "      print('model weights loaded')\n",
        "    except OSError:\n",
        "      print('no previous weights found')\n",
        "    except ValueError:\n",
        "      print('previous weights are different from current') \n",
        " \n",
        "print('\\n\\nTrainning model 2 grade', CURRENT_GRADE)\n",
        "model2.compile(loss='mean_squared_error', optimizer='adam', \n",
        "metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n",
        "model2.fit(xr2, yr2, epochs=60000, batch_size=32, validation_data=(xv2, yv2), \n",
        "verbose=0, callbacks=[callback.LossAndErrorPrintingCallback()])\n",
        " \n",
        "_, accuracy_test_2 = model2.evaluate(xt2, yt2)\n",
        "print('\\n\\nmodel 2 trained')\n",
        "print('\\nAccuracy on test data: %.2f' % (accuracy_test_2))\n",
        " \n",
        "test_predictions = np.around(model2.predict(xt2), 1)\n",
        " \n",
        "if (PRINT_PRDECTIONS):\n",
        "    for i in range(20):\n",
        "        print(' test: predicted:', test_predictions[i], 'real data:', yt2[i])\n",
        " \n",
        "if (SAVE_OUTPUT):\n",
        "    result = np.concatenate([xt2, yt2, test_predictions], axis=1)\n",
        "    data.saveData(result, 'results/omer_data_density.xlsx')\n",
        " \n",
        "if (SAVE_WEIGHTS):\n",
        "    print('\\nsaving model weights ...')\n",
        "    output_dir = os.path.join(os.getcwd(), \"saved_wights\")\n",
        "    model2.save_weights(filepath=os.path.join(output_dir, OUTPUT_TITLE + \"2.h5\"))\n",
        "    print('model weights saved')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading the data ...\n",
            "data loaded with size:  (1300, 17) \n",
            "\n",
            "\n",
            "there is Nan in data\n",
            "\n",
            "\n",
            "shuffling the data ...\n",
            "data shuffled\n",
            "\n",
            "\n",
            "dividing the data ...\n",
            "training data ready with size: (910, 17)\n",
            "test data ready with size: (195, 17)\n",
            "validation data ready with size: (195, 17)\n",
            "data 2 ready with train: (910, 14) and test: (195, 14) and validation: (195, 14)\n",
            "\n",
            "\n",
            "Trainning model 2 grade 30\n",
            "epoch 0 : loss : 5848202.50 , MPE :  100.00. validation loss : 5857171.00 , MPE :  100.00\n",
            "epoch 100 : loss : 4999305.50 , MPE :   97.28. validation loss : 5000571.50 , MPE :   97.26\n",
            "epoch 200 : loss : 3357534.25 , MPE :   91.00. validation loss : 3355280.00 , MPE :   90.96\n",
            "epoch 300 : loss : 1653213.62 , MPE :   82.30. validation loss : 1649983.88 , MPE :   82.25\n",
            "epoch 400 : loss : 435467.25 , MPE :   71.81. validation loss : 433555.44 , MPE :   71.75\n",
            "epoch 500 : loss : 16802.52 , MPE :   60.52. validation loss : 16653.91 , MPE :   60.47\n",
            "epoch 600 : loss : 1028.21 , MPE :   50.73. validation loss :  861.63 , MPE :   50.69\n",
            "epoch 700 : loss : 1028.50 , MPE :   43.62. validation loss :  861.24 , MPE :   43.59\n",
            "epoch 800 : loss : 1028.65 , MPE :   38.29. validation loss :  861.08 , MPE :   38.27\n",
            "epoch 900 : loss : 1028.88 , MPE :   34.14. validation loss :  861.48 , MPE :   34.12\n",
            "epoch 1000 : loss : 1028.97 , MPE :   30.82. validation loss :  860.86 , MPE :   30.81\n",
            "epoch 1100 : loss : 1028.88 , MPE :   28.11. validation loss :  860.20 , MPE :   28.10\n",
            "epoch 1200 : loss : 1028.64 , MPE :   25.85. validation loss :  861.16 , MPE :   25.84\n",
            "epoch 1300 : loss : 1029.84 , MPE :   23.93. validation loss :  861.98 , MPE :   23.92\n",
            "epoch 1400 : loss : 1028.52 , MPE :   22.29. validation loss :  860.80 , MPE :   22.28\n",
            "epoch 1500 : loss : 1028.86 , MPE :   20.87. validation loss :  861.69 , MPE :   20.86\n",
            "epoch 1600 : loss : 1030.20 , MPE :   19.62. validation loss :  861.48 , MPE :   19.62\n",
            "epoch 1700 : loss : 1028.59 , MPE :   18.52. validation loss :  861.08 , MPE :   18.52\n",
            "epoch 1800 : loss : 1030.47 , MPE :   17.55. validation loss :  860.07 , MPE :   17.54\n",
            "epoch 1900 : loss : 1028.69 , MPE :   16.67. validation loss :  860.30 , MPE :   16.67\n",
            "epoch 2000 : loss : 1030.98 , MPE :   15.89. validation loss :  858.16 , MPE :   15.88\n",
            "epoch 2100 : loss : 1029.64 , MPE :   15.18. validation loss :  859.72 , MPE :   15.17\n",
            "epoch 2200 : loss : 1028.76 , MPE :   14.53. validation loss :  861.68 , MPE :   14.53\n",
            "epoch 2300 : loss : 1029.19 , MPE :   13.94. validation loss :  860.39 , MPE :   13.94\n",
            "epoch 2400 : loss : 1028.09 , MPE :   13.40. validation loss :  859.46 , MPE :   13.39\n",
            "epoch 2500 : loss : 1029.38 , MPE :   12.90. validation loss :  859.82 , MPE :   12.90\n",
            "epoch 2600 : loss : 1028.68 , MPE :   12.44. validation loss :  862.46 , MPE :   12.44\n",
            "epoch 2700 : loss : 1030.34 , MPE :   12.01. validation loss :  860.85 , MPE :   12.01\n",
            "epoch 2800 : loss : 1028.88 , MPE :   11.62. validation loss :  861.40 , MPE :   11.62\n",
            "epoch 2900 : loss : 1029.54 , MPE :   11.25. validation loss :  859.37 , MPE :   11.25\n",
            "epoch 3000 : loss : 1029.35 , MPE :   10.91. validation loss :  860.19 , MPE :   10.90\n",
            "epoch 3100 : loss : 1029.36 , MPE :   10.59. validation loss :  861.45 , MPE :   10.58\n",
            "epoch 3200 : loss : 1029.03 , MPE :   10.28. validation loss :  862.45 , MPE :   10.28\n",
            "epoch 3300 : loss : 1028.77 , MPE :   10.00. validation loss :  861.35 , MPE :   10.00\n",
            "epoch 3400 : loss : 1028.81 , MPE :    9.73. validation loss :  861.80 , MPE :    9.73\n",
            "epoch 3500 : loss : 1028.86 , MPE :    9.48. validation loss :  861.44 , MPE :    9.48\n",
            "epoch 3600 : loss : 1030.06 , MPE :    9.25. validation loss :  861.94 , MPE :    9.24\n",
            "epoch 3700 : loss : 1029.32 , MPE :    9.02. validation loss :  862.40 , MPE :    9.02\n",
            "epoch 3800 : loss : 1029.02 , MPE :    8.81. validation loss :  861.39 , MPE :    8.81\n",
            "epoch 3900 : loss : 1029.27 , MPE :    8.61. validation loss :  861.81 , MPE :    8.61\n",
            "epoch 4000 : loss : 1028.38 , MPE :    8.42. validation loss :  860.34 , MPE :    8.41\n",
            "epoch 4100 : loss : 1028.70 , MPE :    8.23. validation loss :  861.48 , MPE :    8.23\n",
            "epoch 4200 : loss : 1029.30 , MPE :    8.06. validation loss :  862.54 , MPE :    8.06\n",
            "epoch 4300 : loss : 1028.78 , MPE :    7.89. validation loss :  860.94 , MPE :    7.89\n",
            "epoch 4400 : loss : 1028.78 , MPE :    7.74. validation loss :  858.78 , MPE :    7.74\n",
            "epoch 4500 : loss : 1029.21 , MPE :    7.59. validation loss :  858.95 , MPE :    7.58\n",
            "epoch 4600 : loss : 1029.44 , MPE :    7.44. validation loss :  860.46 , MPE :    7.44\n",
            "epoch 4700 : loss : 1028.91 , MPE :    7.30. validation loss :  860.47 , MPE :    7.30\n",
            "epoch 4800 : loss : 1029.88 , MPE :    7.17. validation loss :  860.98 , MPE :    7.17\n",
            "epoch 4900 : loss : 1034.20 , MPE :    7.04. validation loss :  867.50 , MPE :    7.04\n",
            "epoch 5000 : loss : 1029.10 , MPE :    6.92. validation loss :  861.14 , MPE :    6.92\n",
            "epoch 5100 : loss : 1029.70 , MPE :    6.80. validation loss :  860.32 , MPE :    6.80\n",
            "epoch 5200 : loss : 1028.64 , MPE :    6.69. validation loss :  860.10 , MPE :    6.69\n",
            "epoch 5300 : loss : 1029.23 , MPE :    6.58. validation loss :  860.16 , MPE :    6.58\n",
            "epoch 5400 : loss : 1029.41 , MPE :    6.48. validation loss :  859.08 , MPE :    6.48\n",
            "epoch 5500 : loss : 1028.55 , MPE :    6.38. validation loss :  860.54 , MPE :    6.38\n",
            "epoch 5600 : loss : 1028.93 , MPE :    6.28. validation loss :  860.77 , MPE :    6.28\n",
            "epoch 5700 : loss : 1030.55 , MPE :    6.19. validation loss :  858.90 , MPE :    6.19\n",
            "epoch 5800 : loss : 1028.51 , MPE :    6.10. validation loss :  861.36 , MPE :    6.10\n",
            "epoch 5900 : loss : 1028.64 , MPE :    6.01. validation loss :  862.19 , MPE :    6.01\n",
            "epoch 6000 : loss : 1030.10 , MPE :    5.93. validation loss :  859.54 , MPE :    5.92\n",
            "epoch 6100 : loss : 1029.13 , MPE :    5.84. validation loss :  863.22 , MPE :    5.84\n",
            "epoch 6200 : loss : 1029.97 , MPE :    5.76. validation loss :  860.39 , MPE :    5.76\n",
            "epoch 6300 : loss : 1029.02 , MPE :    5.69. validation loss :  864.04 , MPE :    5.69\n",
            "epoch 6400 : loss : 1029.66 , MPE :    5.61. validation loss :  860.39 , MPE :    5.61\n",
            "epoch 6500 : loss : 1028.60 , MPE :    5.54. validation loss :  865.46 , MPE :    5.54\n",
            "epoch 6600 : loss : 1029.48 , MPE :    5.47. validation loss :  860.71 , MPE :    5.47\n",
            "epoch 6700 : loss : 1030.06 , MPE :    5.40. validation loss :  863.65 , MPE :    5.40\n",
            "epoch 6800 : loss : 1028.91 , MPE :    5.34. validation loss :  859.70 , MPE :    5.34\n",
            "epoch 6900 : loss : 1028.99 , MPE :    5.28. validation loss :  859.67 , MPE :    5.28\n",
            "epoch 7000 : loss : 1028.73 , MPE :    5.21. validation loss :  860.35 , MPE :    5.21\n",
            "epoch 7100 : loss : 1028.32 , MPE :    5.15. validation loss :  862.17 , MPE :    5.15\n",
            "epoch 7200 : loss : 1028.81 , MPE :    5.09. validation loss :  863.73 , MPE :    5.09\n",
            "epoch 7300 : loss : 1029.82 , MPE :    5.04. validation loss :  860.15 , MPE :    5.04\n",
            "epoch 7400 : loss : 1032.48 , MPE :    4.98. validation loss :  860.28 , MPE :    4.98\n",
            "epoch 7500 : loss : 1028.63 , MPE :    4.93. validation loss :  860.94 , MPE :    4.93\n",
            "epoch 7600 : loss : 1032.72 , MPE :    4.88. validation loss :  864.73 , MPE :    4.88\n",
            "epoch 7700 : loss : 1029.33 , MPE :    4.83. validation loss :  859.53 , MPE :    4.83\n",
            "epoch 7800 : loss : 1029.88 , MPE :    4.78. validation loss :  861.61 , MPE :    4.78\n",
            "epoch 7900 : loss : 1028.73 , MPE :    4.73. validation loss :  862.18 , MPE :    4.73\n",
            "epoch 8000 : loss : 1028.55 , MPE :    4.68. validation loss :  859.41 , MPE :    4.68\n",
            "epoch 8100 : loss : 1029.91 , MPE :    4.63. validation loss :  861.85 , MPE :    4.63\n",
            "epoch 8200 : loss : 1029.05 , MPE :    4.59. validation loss :  859.85 , MPE :    4.59\n",
            "epoch 8300 : loss : 1029.29 , MPE :    4.54. validation loss :  861.87 , MPE :    4.54\n",
            "epoch 8400 : loss : 1029.52 , MPE :    4.50. validation loss :  861.98 , MPE :    4.50\n",
            "epoch 8500 : loss : 1028.01 , MPE :    4.46. validation loss :  862.47 , MPE :    4.46\n",
            "epoch 8600 : loss : 1028.95 , MPE :    4.42. validation loss :  859.93 , MPE :    4.42\n",
            "epoch 8700 : loss : 1028.68 , MPE :    4.38. validation loss :  860.84 , MPE :    4.38\n",
            "epoch 8800 : loss : 1029.27 , MPE :    4.34. validation loss :  861.19 , MPE :    4.34\n",
            "epoch 8900 : loss : 1029.55 , MPE :    4.30. validation loss :  859.18 , MPE :    4.30\n",
            "epoch 9000 : loss : 1029.46 , MPE :    4.26. validation loss :  860.88 , MPE :    4.26\n",
            "epoch 9100 : loss : 1029.17 , MPE :    4.23. validation loss :  860.57 , MPE :    4.23\n",
            "epoch 9200 : loss : 1029.01 , MPE :    4.19. validation loss :  860.07 , MPE :    4.19\n",
            "epoch 9300 : loss : 1030.42 , MPE :    4.16. validation loss :  860.54 , MPE :    4.16\n",
            "epoch 9400 : loss : 1029.21 , MPE :    4.12. validation loss :  860.09 , MPE :    4.12\n",
            "epoch 9500 : loss : 1030.28 , MPE :    4.09. validation loss :  861.01 , MPE :    4.09\n",
            "epoch 9600 : loss : 1028.86 , MPE :    4.06. validation loss :  862.40 , MPE :    4.06\n",
            "epoch 9700 : loss : 1028.97 , MPE :    4.02. validation loss :  860.60 , MPE :    4.02\n",
            "epoch 9800 : loss : 1029.45 , MPE :    3.99. validation loss :  864.24 , MPE :    3.99\n",
            "epoch 9900 : loss : 1029.70 , MPE :    3.96. validation loss :  862.62 , MPE :    3.96\n",
            "epoch 10000 : loss : 1029.35 , MPE :    3.93. validation loss :  862.34 , MPE :    3.93\n",
            "epoch 10100 : loss : 1029.19 , MPE :    3.90. validation loss :  859.88 , MPE :    3.90\n",
            "epoch 10200 : loss : 1028.86 , MPE :    3.87. validation loss :  861.13 , MPE :    3.87\n",
            "epoch 10300 : loss : 1029.55 , MPE :    3.85. validation loss :  860.78 , MPE :    3.85\n",
            "epoch 10400 : loss : 1029.14 , MPE :    3.82. validation loss :  860.33 , MPE :    3.82\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}